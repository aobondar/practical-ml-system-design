# 1. Goals and metrics

## 1.1 Mindset

One of the key traits that sets a strong machine learning engineer with a solid understanding of systems design apart from a novice is the ability to approach problems from an entrepreneur's perspective right from the start. However, the typical work environment for ML engineers does not always foster this type of mindset. In a large enterprise, as engineers often have business leaders who assume responsibility for results, in a startup, the scale of decisions is typically not large enough for engineers to truly feel the weight of responsibility for their engineering choices.

You are fortunate if your manager or the culture within your company encourages accountability for results from the initial draft through to the product's entire lifecycle. In such environments, you naturally absorb the principles we will discuss below. Otherwise, for your professional development, you need to actively cultivate this perspective on your work. So you can ask youself:

> Am I acting as a business leader, who also happens to have the skills to do machine learning in addition to traditional management?

This concept allows you to:
- **Avoid viewing machine learning as the sole solution.** Sometimes, it’s more cost-effective to hire multiple people. Instead of developing a recommendation system, you might set up a content team to curate selections. Rather than implementing a search engine (as self-service solution), you could assign someone to handle the support chat. Any machine learning problem can also be tackled with human effort. Ultimately, it all comes down to project's PnL.
- **Select the most cost-efficient method among all available machine learning approaches.** Even if you decide to setup machine learning system to solve the problem - you'll find some ballance between expenses and quality from business standpoint.
- **Not only integrate machine learning into products and operational processes but also adapt those processes and products to better support machine learning.** TikTok’s recommendations wouldn’t be as effective without the inherent design of the product. Content moderation procedures should capitalize on the advantages of machine learning solutions and address their shortcomings with human intervention.
- **Thoughtfully consider operational support for the solution and manage edge cases.** You are responsible for the overall outcome over the time, which means that all issues related to the project fall under your purview.

The remainder of this brochure applies this mindset to practical scenarios.

## 1.2 Goal

In real-world scenarios, engineers often receive requests like, "Could you please create a model that does X?" However, it’s important to trace those requests back to the original problem. For example:

- *"I need a recommendation system"* really means "I want to increase sales through more personalized offers."
- *"I need a classifier for NSFW content"* translates to "I want to reduce the number of incidents involving inappropriate content in our user-generated content system."
- *"I need a RAG system for our headquarters"* indicates "I want to cut down the time employees spend searching our intranet."

Understanding the underlying goals provides you with more flexibility and a higher chance of success.

Even if a business stakeholder presents a solid design idea, you still need a clear definition of the ultimate goal to:

- Prioritize the initiative relative to other opportunities and ensure it aligns with core business objectives.
- Analyze in detail the potential effects that could be achieved by building an ideal model with 100% accuracy.
- Estimate the financial impact to eliminate solutions that would lead to negative profitability.
- Develop a metrics system for tracking your progress and assessing the overall impact.
- Rapidly create a quick and rough prototype (often non-ML) as a baseline for your model during its development.
- And, importantly, evaluate whether you ethically agree to undertake the task.

While all of this may seem obvious, it is surprisingly easy to lose sight of the original goal and become distracted by secondary objectives. It’s best to avoid this pitfall to prevent pouring time and effort into a project that ultimately benefits no one.

It's true, that many of these tasks typically fall under the purview of product managers, analysts, and business leaders. However, if it becomes clear that these responsibilities are being neglected by your colleagues, then your engineering efforts may be rendered ineffective. To mitigate this risk and contribute to the team's success, you can take a proactive approach by ensuring these critical questions are addressed before commencing the actual development phase.

In commercial organizations, the primary goal is to generate profits, which means your objectives will likely fall into one of the following categories:

- **Increase Revenue**: This can entail launching paid ML-based services for B2B or B2C markets or adopting systems that enhance sales through personalization. Ultimately, the success hinges on the direct revenue generated by these solutions. This objective is key when the company has high profit margins or aims at market expansion. In high-margin scenarios, increased revenue directly boosts the company's profits — common in the software industry where logistic expensess doesn't grow same speed as sales volume. Conversely, in market expansion, higher revenue might initially result in losses; however, stakeholders anticipate that this strategy will lead to market dominance, enabling the company to raise prices and recover the early losses.

- **Save Money**: For instance, by automating routine tasks in call centers or document management processes. This goal is important for large, traditional businesses with low profit margins, where each additional dollar in sales yields only a few extra cents in profit. In these cases, focusing on efficiency can yield significantly greater returns. In real-world scenarios, measuring the effects of automation is not straightforward. You either need to maintain two versions of the business process: one with automation and one without (which can be costly), or you make hypothetical assumptions about "how much more money we would spend without automation" (which can be quite theoretical). A more or less honest evaluation can be done by comparing the EBITDA of the company to industrial benchmarks, but it obviously doesn’t provide clear insights into which particular projects provided this effect.

- **Grow active userbase**: This usually involves developing systems that engage and retain users within a product, with the plan to eventually monetize through other paid services. If the initiative is new, the link between user retention and future revenue heavily depends on management's beliefs. For more mature initiatives, these ecosystem effects are typically well-calculated. In such projects, it is essential to acknowledge that there will come a day when stakeholders ask, "Are we really making money from this?" This question is worth considering from the outset.

Ultimately, especially over a horizon of five to ten years, the results of your work will be judged based on their impact on the financial health of the company. During periods of economic growth, stakeholders tend to trust projects that promise returns over time or indirectly. In times of economic downturn, they will be more inclined to support projects that generate immediate revenue. However, in the end, every project will be evaluated through its effect on financial streams, which is something to keep in mind when designing a new and exciting system.

## 1.3 Metrics

> A good metric:
> * Everyone trusts (or it's checked) that increasing this metric leads to improvements in financial metrics
> * Is calculated by external oracle (not yourself)
> * It allows distignuish effect of your system from other effects
> * The measurement is cheap enough
> * Is sensetive enough to capture a magnitude of your changes
> * Is ballanced by boundary metric
>
> It's all about trade-offs.

Once you have a clear understanding of your goal, it’s important to establish a metric to track the impact of your work on the business. An ideal metric should:

* **Objectively measure the effect on business**. Both you and your stakeholders should view this metric as an objective measure of success. It’s crucial to align on this from the very beginning and update any agreements regarding the metrics if things change. The closer the metric is to actual profits, the easier it will be for you to assess its real impact on the business. On the other hand, the further the metric is from financial outcomes, the more you will rely on stakeholders' beliefs about the connection between the metric and an actual effect. A significant challenge lies in ensuring everyone has a unified understanding of the metrics. For example when engineer reports 99% accuracy on holdout set business leaders tend to interpret this number as accuracy on production at any given moment. Or F1 score is casually interpreted as accuracy. So engineers should make extra steps on making sure everyone in the room understands metrics equally.

* **Allow for effect attribution to specific systems**. This means you should be able to clearly identify how much of the change in the metric was caused by your system, excluding external factors. For example EBITDA is a great metric for tracking down business efficiency, but in complex business scenarios you can hardly do direct attribution to a specific activities when this metric changes.

* **Cheap enough to measure**. Beyond financial costs, there are also time expenditures, missed alternative opportunities or harmed user experience to consider. For example LTV and retention are great metrics for your product initiatives, but A/B tests on these metrics take a lot of time. Or you can measure user satisfaction using CSAT or NPS, but simply conducting these surveys can reduce customer satisfaction (who loves doing survays anyway?). And the most important: while you're doing A/B a cohort of users is getting worse service no matter of A/B results.

* **Is sensitive enough to measure your changes** For example let's assume we have fixed userbase of 300 employees. On this userbase we can't do A/B tests for CTR of recommendation system, with base value 5% to track 0.5% changes. Also obviously holdout made of 100 datapoints can't measure 1% metrics change with any reasonable confidence.

* **Rely on external oracle**. In high-stakes scenarios external evaluation cultivates trust and eliminates conflict of interests right from the beginning. Is measurement is done by engineer and there is a mistake, then it can look like a fraud (even if it was a honest mistake). Therefore, it's better for the engineer to have an independent source to assess the success of their work.

Reality makes it more complicated:
* Out of academia there aren't metrics, which meet all the above criteries
* The problems you work on usually way too complex to be fully quantified with a single metric

Second statement needs more clarification. In business context solution space for a problem often has at least two dimensions and it's hard to collapse them into a single metric. For example:
* **quatily of service** vs **service cost**. For example, in customer support, you could hire less educated employees or automate with annoying robots. This would reduce costs, but it could have consequences on user retention and future revenue. Since quality is a long-term proxy metric for future profits, it's almost impossible to combine it with current expenses into a single metric without far-fetched assumptions.
* **advertisment net profits** vs **user retention**. If the business model relies on ad monetization, then showing more ads will definitely boost profit, but on the other hand, it may annoy users and increase the risk of them dropping out.

* **inappropriate content coverage** vs **cost of moderation service**. A common bottlenack of any content moderation system - is a moderator's throughtput. To address this, one could increase it, which raises the overall cost, or decrease it, which leads to an increase in the number of incidents.

This dichotomy encapsulates the core challenge of doing profitable business: risk management. Ventures that are certain to succeed usually offer lower profitability. Therefore, while you may want to take risks, you aim for them to be moderate. Since machine learning models solve business problems, we encounter this complexity as well.

There are two common patterns for working with such a system of metrics:

* **Building a synthetic metric.** The F1 score is a textbook example of a synthetic metric, combining more direct measures like precision and recall. One obvious drawback is that this metric often lacks direct interpretation. You have a pretty clear understanding of what a +0.05 increase in precision means (for every 100 positive classifications, you'd expect 5 more true positives). Similarly, you can interpret an increase in recall. But it's not completely apparent what a +0.05 increase in the F1 score means to you. However, in some cases, you can retain interpretability even for a synthetic metric. For instance, if your metric needs to combine financial losses now with expected gains later, you can discount future gains and directly sum them up with current losses to create a single financial metric.
* **Target + boundary metric.** More often, it's feasible to select a key metric to focus on and establish a boundary level for a secondary metric that you don't want to exceed. For example, you may want to increase the level of automation without degrading quality. This focus helps with the next steps of project development.

Here are some actual examples of metrics:
| Goal                             | METRIC                                                  | Trustworthy | Gives Attribution                                                  | Cheap                                                          | Sensitive | External Oracle |
|----------------------------------|---------------------------------------------------------|-------------|--------------------------------------------------------------------|----------------------------------------------------------------|-----------|-----------------|
| Increase level of automation     | Change in cost per customer for groups with and without the bot | Excellent   | Good (via A/B testing, though it might be affected by behavior differences in users with/without the bot) | OK (some people receive a degraded service, data collection takes time) | Great     | Yes             |
|                                  | Saved operator's time (comparison with users without automation) | Great (excellent if all operators are paid equally)               | Great (via A/B testing, though it might be affected by behavior differences in users with/without the bot) | OK (some people receive a degraded service, data collection takes time) | Great     | Yes             |
|                                  | Number of fully automated dialogues                        | Good (some dialogs are easier to automate than others)           | Great (via A/B testing, though it might be affected by behavior differences in users with/without the bot) | Good (automatically generated, but time for data collection is needed) | Great     | Yes             |
|                                  | Percentage of correctly classified intents (random sample annotation from the request stream) | Proxy (accurate intent classification should lead to correct procedure) | Great, checks only the intent classifier's system properties (assuming ideal annotation quality)      | OK (manual data annotation is needed)                                | Great     | OK (it still impacts the measurement even if another team is involved) |
|                                  | Train-test split on training data                         | Bad         | Great, checks only the intent classifier's system properties (assuming ideal annotation quality)      | Great                                                          | Great     | No (you have direct impact on the measurement) |
|                                  |                                                         |             |                                                                      |                                                                |           |                 |
| Keep the quality of service      | Net Promoter Score (NPS)                                 | Excellent   | No (it could be influenced by any business aspect)                | No (time-consuming to collect, potentially worsens user experience) | Bad (feedback is rarely given) | Yes             |
|                                  | CSAT / likes-dislikes on messages                        | Great       | OK (unclear if the issue is with robot support or the business process in general) | No (time-consuming to collect, potentially
