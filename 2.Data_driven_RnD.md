# Data Driven R&D

## 2.1 Key goal of R&D

Machine learning is not a guaranteed recipe for solving a problem. You essentially make the assumption that the given data contain patterns which can assist you in solving your task, and that a machine learning algorithm is capable of learn them. Around this fundamental question of "can it be solved by ML at all?" you also face a variety of choices in terms of architectures, feature engineering, hyperparameter tuning, and the overall system design. This places you in the realm of R&D rather than conventional engineering.

The good news is that often your business leader actually also wants to test a product hypothesis (as well as you need to test ML ones). For instance, that personalization generally leads to additional sales, or that by improving search quality, user retention within the service can be achieved. In this case, there is no need to separate technological hypotheses from business ones. And as we will see later, testing a business hypothesis often means going through all reasonable technical efforts within the scope that a business stakeholder is willing to invest. However, sometimes it's much cheaper to figure out how to test a business hypothesis without delving deep into technological R&D.

Your goal in that kind of project is to list all reasonable combinations of different approaches and parameters of solving a problem, order them in terms of expected gains, divided by complexity and run through them as quick as possible untill you meet your DoD.

When you list all your hypothesis your primary goal should be to narrow down the hypothesis space as quickly as possible. There are two key tools to help you do this:

* Accelerate iterations and rapidly work through the list of hypotheses.
* Conduct offline analytics to reduce the list of hypotheses.

In pratcice you usually combine these two methods in order to have the best results. Let's consider two big projects, and how to setup R&D process around them, and then we'll generalize all the tools we've used to achieve success.

## 2.2.1 Example 1: customer support automation

Let's assume your setup is:
* Your key hypothesis is that you can reduce costs on customer support by automating some of user requests
* Your metric is reducion in time-spent by operators on solving user problems (via A/B tests with vs without automation) with keepeing difference in quality (CSAT: 1 to 5) not bigger than 0.3
* You consider project successful (ROI > 10) if your team achieve 30% of automation within 1 year

From technical standpoint you have a list of industry-adopted aproaches for support automation:
* Smart rouing within contact-center
* Self-service FAQ
* Intent classification + procedures with slot-fillings
* RAG
* LLM-agents
* Communication history summarization for operator
* Any combination of all above solutions

And all these hypothesis actually incapsulate a ton of smaller hypothesis and technical decisions:

**Smart rouing within contact-center:** 
* Is there enough inefficiency in random routing?
* What is model architecture? (xgboost, linear, nearual-network)
* What are the features (user features, operator features, communication topic features, etc)?
* Will it be accurate enough to do uplift?
* What is exploration strategy for new operators (random calls? contextual bandits? etc?)

**Self-service FAQ**
* Are there enough non-personalized and popular questions that could be answered with static article?
* Will users use self-service instead of writing to operator?
* How to collect articles, that cover requests stream? (take only existing, modify them? do clustering of user requests to find new popular topics? what is clustering model and hyperparameters?)

**Intent classification + procedures with slot-fillings**
* Are there enough popular topics and support scenarios, or every single request is unique?
* What is an architecture for intent classification which gives best cost/quality ratio? (regexps, kNN, transformer encoder, LLM-based) How much fine-tuning should be done before saturation on that particular distribution? What is rejection of classification strategy and how it's tuned?
* What are slot-filling models to be used which gives best cost/quality ratio? (regexps, NER + normalizers, LLM-based)
* What is operational process which finds and creates new intents? How often should intent-classifier be re-trained? On slots? Is system sustainable to data drifts?

**RAG**
* Are there enough customer issues, which can be covered by static documentation? (for example if user want to do action, the documentation won't help them much)
* Do we have enough writen knowledge bases, which covers some noticable part of traffic? Can we write documentation for the rest in reasonable time?
* Will LLM be able to do factually correct answers without exceeding accepted error rate (by the way what is acceptable error rate)? Which one? Should we fine-tune it? With how much data? Do we need guardrails to prevent excessive hallucinations? What's going to be a tradeoff between quality and coverage in that case?
* What's going to be a trigger to reject answering with RAG?
* Is system sustainable to data drifts?

**LLM-agents**


**Communication history summarization for operator**
  
As you can imagine discovering the answers to all those questions can be pretty costly. Good thing is that you don't need to. You can actually sort them by risk (most risky - you try to solve unexisting problem, least risky - you've selected suboptimal hyperparameter during training), and first of all evaluate key hypothesis.

In that particular case you can seem that all approaches have almost the same assumption: it suggests that there are big clusters of non-unique user questions (or unique, but covered by KB). So you actually can evaluate biggest risks for all approaches at once with analytics. Let's check, for example, if intent-based approach is reasonable:
* Take **300** user requests to customer support from production distribution
* Try to manually cover them with **10-30** topics (intents)
* Try to define support flow for that intents, which covers all the cases, attached
* Mark all the intents as "homogenious flow", "unique questions", "something in between"
* Now sum up all the cases, which is covered by intents with "homogenious flow" - this is your ceiling effect estimation.
<img width="1099" alt="image" src="https://github.com/user-attachments/assets/e41a7dc2-7679-4c37-8555-5bada357d5fa" />
If this effect estimation is higher than your targer (**30%**), then it make sense to start R&D for building intent-based dialogue system.

In case of RAG hypothesis, since you can build proof of concept in days, you can actually delve into more detailed analytics, by applying baseline solution to random samples from prod distributions.


## 2.2.2 Example 2: recommendation system for UGC

## 2.2.3 Example 3: general purpose LLM-copilot for in-house enterprise usage

## 2.3 Key takeaways


What to cover:

- Narrowing down your hypothesis space and 'unknown zone' (on RAG and Agents examples). Recsys (on )
RAG: requests space? which type of index? what documents base? take existing or rewrite? summarizer? hyperparameters?
Recsys: are there good signals? what are the features? what is the model architecture?
LLM: can we do distributed training? do we have clean anough dataset? which data is required? ect
Search: ----
Agents: are working at all? Can we distignuish good and bad answers? Which model to take? Which tools?

Two ways for efficiency:
* reduce your hypothesys space by analytics (recsys - corellations, nlp - annotate examples)
* speed up you experiments cycle

Estimate and setup quick iteration speed. Fastest: online metrics with quick feedback, slow online metrics / slow criteria + fast offline. Slow online, slow release cycle, slow offline - not much experiments, makes sense to have something more straightforward. Fast experiments = more risky approaches.

What R&D roadmap is:
- hypothesis list (implement & run or analyze)
- improvements of experiments setup
- delivery to production
