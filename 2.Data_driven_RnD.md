# Data Driven R&D

## 2.1 Key goal of R&D

Machine learning is not a guaranteed recipe for solving a problem. You essentially make the assumption that the given data contain patterns which can assist you in solving your task, and that a machine learning algorithm is capable of learn them. Around this fundamental question of "can it be solved by ML at all?" you also face a variety of choices in terms of architectures, feature engineering, hyperparameter tuning, and the overall system design. This places you in the realm of R&D rather than conventional engineering.

The good news is that often your business leader actually also wants to test a product hypothesis (as well as you need to test ML ones). For instance, that personalization generally leads to additional sales, or that by improving search quality, user retention within the service can be achieved. In this case, there is no need to separate technological hypotheses from business ones. And as we will see later, testing a business hypothesis often means going through all reasonable technical efforts within the scope that a business stakeholder is willing to invest. However, sometimes it's much cheaper to figure out how to test a business hypothesis without delving deep into technological R&D.

When you try to apply a solution to a problem usually you rely on some assumptions. Some assumptions (when being wrong) can be show stoppers, and some of them can just move you to suboptiomal solutions. Your key goal here is identify them, and test against reality as quickly as possible. From most fundamental (If we make ideal model with 100% accuracy - will it solve a problem we have?) to the tiny details (Is 1e-5 right realrning rate?). From cheapest (let's try to solve it without ML at all) to most complicated and expensive (let's establish LLM-agents system on a large scale for end-users).

When you list all your hypothesis your primary goal should be to narrow down the hypothesis space as quickly as possible. There are two key tools and one principle to help you do this:

* Accelerate iterations and rapidly work through the list of assumptions.
* Conduct analytics to reduce the list of hypotheses.
* Fail fast: check the most fundamental risk first, and drop the whole experiments cycle if your core assumption is wrond.

In pratcice you usually combine all of these in order to have the best results. Let's consider seleral typical projects, and how to setup R&D process around them. Then we'll generalize all the tools and tricks we've used to achieve success.

## 2.2.1 Example 1: customer support automation

Let's assume your setup is:
* Your key hypothesis is that you can reduce costs on customer support by automating some of user requests
* You use "percentage of fully automated dialogs" as a proxy to cost savings, and you have a control group of users without any automation to compare customer satisfatction (1<CSAT<5).
* Project goal is to achieve **30%** of automation within 1 year, keeping customer satisfaction delta to control group higher than **0.1**.
> [!NOTE]
> (One can argue here, that it's not a real goal - real goal is in time-saving, which can be achieved also by smart routing or co-pilots for operators. In sake of simplicity we'll keep aside these opportunities in this narrative, but this point is totally legit).

From technical standpoint you have a list of industry-adopted aproaches for support automation:
* Self-service FAQ
* Intent classification + procedures with slot-fillings
* RAG
* LLM-agents (you list all given tools, and support reglaments to LLM via prompting, and let LLM solve customer's issue)
* Any combination of all above solutions

Choosing any of these aproaches still opens a lot of questions to answer, before you can answers for sure "this approach can bring desired results". For example:

**Self-service FAQ**
Assumption here is that we can collect most common customers' problems (and update that list in reasonable time) for our products, leave non-personalized self-service recipe for all that problems, and customers on their own will find article and solve their problem without additional help.

<details><summary>Key hypothesis to check here</summary>

* Are there enough non-personalized and popular questions that could be answered with static article to cover 30% of user requests?
* Will users use self-service instead of writing to operator?
* How to collect articles, that cover requests stream? (take only existing, modify them? do clustering of user requests to find new popular topics? what is clustering model and hyperparameters?)

</details>


**Intent classification + procedures with slot-fillings**
Assumption here is that there are clusters of user problems, which are solved identically by the same business process. The differ only in brief, given by user (can miss some improtant details, or just say "I need help") and solution parametrization (for example if user want to issue a credit card - you can suggest several options).

<details><summary>Key hypothesis to check here</summary>

  * Are there enough popular topics and support scenarios to cover 30% of user requests?
* What is an architecture for intent classification which gives best cost/quality ratio? (regexps, kNN, transformer encoder, LLM-based) How much fine-tuning should be done before saturation on that particular distribution? What is rejection of classification strategy and how it's tuned?
* What are slot-filling models to be used which gives best cost/quality ratio? (regexps, NER + normalizers, LLM-based)
* What is operational process which finds and creates new intents? How often should intent-classifier be re-trained? On slots? Is system sustainable to data drifts?
* What is a cost of operational process to achieve desired results? Do we have enough time and personell to achieve it?

</details>

**RAG**
Assumption here is that there are clusters of user problems, which are consultational in nature (you don't have to do anything on behalf of user in our system). And all required information is given in our knowledge base. So we can retrieve it and summarize with LLM to give self-service solution.

<details><summary>Key hypothesis to check here</summary>

* Are there enough customer issues, which can be covered by static documentation to cover 30% of user requests? (for example if user want to do action, the documentation won't help them much)
* Do we have enough writen knowledge bases, which covers some noticable part of traffic? Can we write documentation for the rest in reasonable time and costs?
* What is a retrieval architecture to implement? Which one gives the best recall@N? What is reasonalbe N here?
* Will LLM be able to do factually correct answers without exceeding accepted error rate (by the way what is acceptable error rate)? Which one? Should we fine-tune it? With how much data? Do we need guardrails to prevent excessive hallucinations? What's going to be a tradeoff between quality and coverage in that case?
* What's going to be a trigger to reject answering with RAG?
* Do we need to add some personalized data to LLM prompt to answer correctly additionally to KB chunks?
* Is system sustainable to data drifts?

</details>

**LLM-agents**
Assumption here is that we can prompt LLM with actual business processes and tools, required to implement that policies, and the model will be able to solve them following business processes and on the other hand will pass the batton to real operator, if the business process is not specified in prompt.

<details><summary>Key hypothesis to check here</summary>

* Can we cover enough business cases with prompt and tooling to achieve desired automation level? Do we have all tools required to do that as documented API calls? Do we have all the processes described? How much efforts does it take to write them down if it's not written yet?
* 

</details>

It's quite a lot of options. So let's consider them one by one, how can we reduce amount of "degrees of freedom" in that task.

### Hypothesis 1. Self-service FAQ.
This is the cheapest solution, so we're obviously want to try it out. If we have a FAQ: let's just plug it in customer support flow. If we don'tm then let's user some clustering techniques to build FAQ. What can go wrong?
* 1. Every single users' request is a unique problem, which can't be addressed by self-service non-presonalized instruction.
* 2. Users wouldn't be able (or want to) find answer in FAQ and will directly go to the human operator (even when FAQ could solve their problem).
* 3. We can't build efficient enough clustering process to cover enough user problems to achieve and keep **30%** of automation
* 4. Users will have too low CSAT on self-service scenario

Let's think, how can we test that risks and quick. The most fundamental risk here is the first one: that we don't have enough user requests in data, which can **even in theory** be solved by FAQ. This risk is fundamental, because if it's true, than even if in theory all other risks are false, the project can't be done. Good news here is that we can estimate this risk by doing simple analytics:
* Take **1000** users' dialogs with customer support from production distribution (it will give around **3%%** of variance in your estimations)
* Manually annotated it for 3 classes: "Already have FAQ article for this case" | "We can write FAQ article for that case" | "Can't be solved with FAQ"
<img width="1000" alt="image" src="https://github.com/user-attachments/assets/6d7dbd42-887d-4b63-8c67-7f40aaaf4c07" />

Now if your green and yellow zones in sum doesn't give us 30%% of automation, we can shut down this track **right here**.

If not, we can make our estimations more precise. Because of risks 2 and 3 we can't expect have nor **49%** nor **16%** of automation. To have better understanding of this we can try to run pilot with already existing FAQ articles. Let's do that, and annotate random sample of user iteractions for 3 classes: "Automated (Checked FAQ and never called operator)" | "Ignored FAQ (we have solution in FAQ but user ignored it) | "Don't have FAQ article for that".

<img width="998" alt="image" src="https://github.com/user-attachments/assets/efe84e08-d7cd-45f3-a25e-9e13a2c835d3" />

Now you can make rough estimation of your total effect here. If it's lower than your target - then apparently this architecture can't be a sole solution to a problem. But let's assume, that 

As you can imagine discovering the answers to all those questions can be pretty costly. Good thing is that you don't need to. You can actually sort them by risk (most risky - you try to solve unexisting problem, least risky - you've selected suboptimal hyperparameter during training), and first of all evaluate key hypothesis.

In that particular case you can seem that all approaches have almost the same assumption: it suggests that there are big clusters of non-unique user questions (or unique, but covered by KB). So you actually can evaluate biggest risks for all approaches at once with analytics. Let's check, for example, if intent-based approach is reasonable:
* Take **300** user requests to customer support from production distribution
* Try to manually cover them with **10-30** topics (intents)
* Try to define support flow for that intents, which covers all the cases, attached
* Mark all the intents as "homogenious flow", "unique questions", "something in between"
* Now sum up all the cases, which is covered by intents with "homogenious flow" - this is your ceiling effect estimation.
<img width="1099" alt="image" src="https://github.com/user-attachments/assets/e41a7dc2-7679-4c37-8555-5bada357d5fa" />

If this effect estimation is higher than your targer (**30%**), then it make sense to start R&D for building intent-based dialogue system.

In case of RAG hypothesis, since you can build proof of concept in days, you can actually delve into more detailed analytics, by applying baseline solution to random samples from prod distributions.


## 2.2.2 Example 2: recommendation system for UGC

## 2.2.3 Example 3: general purpose LLM-copilot for in-house enterprise usage

## 2.3 Key takeaways


What to cover:

- Narrowing down your hypothesis space and 'unknown zone' (on RAG and Agents examples). Recsys (on )
RAG: requests space? which type of index? what documents base? take existing or rewrite? summarizer? hyperparameters?
Recsys: are there good signals? what are the features? what is the model architecture?
LLM: can we do distributed training? do we have clean anough dataset? which data is required? ect
Search: ----
Agents: are working at all? Can we distignuish good and bad answers? Which model to take? Which tools?

Two ways for efficiency:
* reduce your hypothesys space by analytics (recsys - corellations, nlp - annotate examples)
* speed up you experiments cycle

Estimate and setup quick iteration speed. Fastest: online metrics with quick feedback, slow online metrics / slow criteria + fast offline. Slow online, slow release cycle, slow offline - not much experiments, makes sense to have something more straightforward. Fast experiments = more risky approaches.

What R&D roadmap is:
- hypothesis list (implement & run or analyze)
- improvements of experiments setup
- delivery to production
