# Data Driven R&D

## 2.1 Key goal of R&D

Machine learning is not a guaranteed recipe for solving a problem. You essentially make the assumption that the given data contain patterns which can assist you in solving your task, and that a machine learning algorithm is capable of learn them. Around this fundamental question of "can it be solved by ML at all?" you also face a variety of choices in terms of architectures, feature engineering, hyperparameter tuning, and the overall system design. This places you in the realm of R&D rather than conventional engineering.

The good news is that often your business leader actually also wants to test a product hypothesis (as well as you need to test ML ones). For instance, that personalization generally leads to additional sales, or that by improving search quality, user retention within the service can be achieved. In this case, there is no need to separate technological hypotheses from business ones. And as we will see later, testing a business hypothesis often means going through all reasonable technical efforts within the scope that a business stakeholder is willing to invest. However, sometimes it's much cheaper to figure out how to test a business hypothesis without delving deep into technological R&D.

Your goal in that kind of project is to list all reasonable combinations of different approaches and parameters of solving a problem, order them in terms of expected gains, divided by complexity and run through them as quick as possible untill you meet your DoD.

When you list all your hypothesis your primary goal should be to narrow down the hypothesis space as quickly as possible. There are two key tools to help you do this:

* Accelerate iterations and rapidly work through the list of hypotheses.
* Conduct offline analytics to reduce the list of hypotheses.

In pratcice you usually combine these two methods in order to have the best results. Let's consider two big projects, and how to setup R&D process around them, and then we'll generalize all the tools we've used to achieve success.

## 2.2.1 Example 1: customer support automation

Let's assume your setup is:
* Your key hypothesis is that you can reduce costs on customer support by automating some of user requests
* Your metric is reducion in time-spent by operators on solving user problems (via A/B tests with vs without automation) with keepeing difference in quality (CSAT) not bigger than 0.3
* You consider project successful (ROI > 10) if your team achieve 30% of automation within 1 year

From technical standpoint you have a list of industry-adopted architectures for chatbot building:
* Smart rouing within contact-center
* Self-service FAQ
* Intent classification + procedures with slot-fillings
* RAG
* LLM-agents
* Communication history summarization for operator
* Any combination of all above solutions

And all these hypothesis actually incapsulate a ton of smaller hypothesis and technical decisions:

**Smart rouing within contact-center:** 
* Is there enough inefficiency in random routing?
* What is model architecture? (xgboost, linear, nearual-network)
* What are the features (user features, operator features, communication topic features, etc)?
* Will it be accurate enough to do uplift?
* What is exploration strategy for new operators (random calls? contextual bandits? etc?)

**Self-service FAQ**
* Are there enough non-personalized and popular questions that could be answered with static article?
* Will users use self-service instead of writing to operator?
* How to collect articles, that cover requests stream? (take only existing, modify them? do clustering of user requests to find new popular topics? what is clustering model and hyperparameters?)

**Intent classification + procedures with slot-fillings**
* Are there enough popular topics and support scenarios, or every single request is unique?
* What is an architecture for intent classification which gives best cost/quality ratio? (regexps, kNN, transformer encoder, LLM-based) How much fine-tuning should be done before saturation on that particular distribution?
* What are slot-filling models to be used which gives best cost/quality ratio? (regexps, NER + normalizers, LLM-based)
* What is operational process which finds and creates new intents? How often should intent-classifier be re-trained? On slots?

**RAG**
**LLM-agents**
**Communication history summarization for operator**
**Any combination of all above solutions**
  
As you can see all that hypothesis seems pretty reasonable. But just fully implementing them and all their combinations is a pretty costly, so at first let's consider their implementation costs (can we make a quick prototype and evaluate effect?) and possible analytics (can we quickly disproof hypothesis?)

Smart rouing within contact-center


## 2.2.2 Example 2: recommendation system for UGC

## 2.2.3 Example 3: general purpose LLM-copilot for in-house enterprise usage

## 2.3 Key takeaways


What to cover:

- Narrowing down your hypothesis space and 'unknown zone' (on RAG and Agents examples). Recsys (on )
RAG: requests space? which type of index? what documents base? take existing or rewrite? summarizer? hyperparameters?
Recsys: are there good signals? what are the features? what is the model architecture?
LLM: can we do distributed training? do we have clean anough dataset? which data is required? ect
Search: ----
Agents: are working at all? Can we distignuish good and bad answers? Which model to take? Which tools?

Two ways for efficiency:
* reduce your hypothesys space by analytics (recsys - corellations, nlp - annotate examples)
* speed up you experiments cycle

Estimate and setup quick iteration speed. Fastest: online metrics with quick feedback, slow online metrics / slow criteria + fast offline. Slow online, slow release cycle, slow offline - not much experiments, makes sense to have something more straightforward. Fast experiments = more risky approaches.

What R&D roadmap is:
- hypothesis list (implement & run or analyze)
- improvements of experiments setup
- delivery to production
