# Data Driven R&D

## 2.1 Key goal of R&D

Machine learning is not a guaranteed recipe for solving a problem. You essentially make the assumption that the given data contain patterns which can assist you in solving your task, and that a machine learning algorithm is capable of learn them. Around this fundamental question of "can it be solved by ML at all?" you also face a variety of choices in terms of architectures, feature engineering, hyperparameter tuning, and the overall system design. This places you in the realm of R&D rather than conventional engineering.

Your goal in that kind of project is to list all reasonable combinations of different approaches and parameters of solving a problem, order them in terms of expected gains, divided by complexity and run through them as quick as possible untill you meet your DoD.

When you list all your hypothesis your primary goal should be to narrow down the hypothesis space as quickly as possible. There are two key tools to help you do this:

* Accelerate iterations and rapidly work through the list of hypotheses.
* Conduct offline analytics to reduce the list of hypotheses.

In pratcice you usually combine these two methods in order to have the best results. 

## 2.2 Top-down R&D planning

## 2.2 Faster iterations

## 2.3 Doing

## 2.4 Examples os R&D roadmaps


What to cover:

- Narrowing down your hypothesis space and 'unknown zone' (on RAG and Agents examples). Recsys (on )
RAG: requests space? which type of index? what documents base? take existing or rewrite? summarizer? hyperparameters?
Recsys: are there good signals? what are the features? what is the model architecture?
LLM: can we do distributed training? do we have clean anough dataset? which data is required? ect
Search: ----
Agents: are working at all? Can we distignuish good and bad answers? Which model to take? Which tools?

Two ways for efficiency:
* reduce your hypothesys space by analytics (recsys - corellations, nlp - annotate examples)
* speed up you experiments cycle

Estimate and setup quick iteration speed. Fastest: online metrics with quick feedback, slow online metrics / slow criteria + fast offline. Slow online, slow release cycle, slow offline - not much experiments, makes sense to have something more straightforward. Fast experiments = more risky approaches.

What R&D roadmap is:
- hypothesis list (implement & run or analyze)
- improvements of experiments setup
- delivery to production
